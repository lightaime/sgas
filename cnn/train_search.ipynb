{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributions.categorical as cate\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from model_search import Network\n",
    "from architect import Architect\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_CLASSES = 10\n",
    "\n",
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('--data', type=str, default='../data', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=6, help='batch size')\n",
    "parser.add_argument('--batch_increase', default=8, type=int, help='how much does the batch size increase after making a decision')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
    "parser.add_argument('--learning_rate_min', type=float, default=0.001, help='min learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--init_channels', type=int, default=16, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
    "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.3, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=2, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
    "parser.add_argument('--unrolled', action='store_true', default=False, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "parser.add_argument('--warmup_dec_epoch', type=int, default=9, help='warmup decision epoch')\n",
    "parser.add_argument('--decision_freq', type=int, default=5, help='decision freq epoch')\n",
    "parser.add_argument('--use_history', action='store_true', help='use history for decision')\n",
    "parser.add_argument('--history_size', type=int, default=4, help='number of stored epoch scores')\n",
    "parser.add_argument('--post_val', action='store_true', default=False, help='validate after each decision')\n",
    "#args = parser.parse_args()\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def load_data(args):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "    train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
    "    _, train_data, valid_data = torch.utils.data.random_split(train_data, [30000, 10000, 10000])\n",
    "    test_data = dset.CIFAR10(root=args.data, train=False, download=True, transform=valid_transform)\n",
    "\n",
    "    train_queue = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True, num_workers=2)\n",
    "\n",
    "    valid_queue = torch.utils.data.DataLoader(\n",
    "        valid_data, batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True, num_workers=2)\n",
    "    \n",
    "    test_queue = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True, num_workers=2)\n",
    "    \n",
    "    return train_queue, valid_queue, test_queue\n",
    "'''\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    torch.cuda.empty_cache()\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5,), (0.5,)),])\n",
    "    \n",
    "    train_data = datasets.MNIST(root=args.data, download=True, train=True, transform=transform)\n",
    "    _, train_data, test_data = torch.utils.data.random_split(train_data, [58000, 1000, 1000])\n",
    "    valid_data = datasets.MNIST(root=args.data, download=True, train=False, transform=transform)\n",
    "\n",
    "    train_queue = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True, num_workers=2)\n",
    "\n",
    "    valid_queue = torch.utils.data.DataLoader(\n",
    "        valid_data, batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True, num_workers=2)\n",
    "    \n",
    "    test_queue = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True, num_workers=2)\n",
    "    \n",
    "    return train_queue, valid_queue, test_queue\n",
    "\n",
    "def train(train_queue, valid_queue, model, architect, criterion, optimizer, lr, epoch):\n",
    "    objs = utils.AverageMeter()\n",
    "    top1 = utils.AverageMeter()\n",
    "    top5 = utils.AverageMeter()\n",
    "    \n",
    "    pbar = enumerate(train_queue)\n",
    "    print(('\\n' + '%10s' * 4) % ('Epoch', 'Loss', 'Top1', 'Top5'))\n",
    "    pbar = tqdm(pbar, total=len(train_queue))\n",
    "    \n",
    "    for step, (input, target) in pbar:\n",
    "        model.train()\n",
    "        n = input.size(0)\n",
    "\n",
    "        input = Variable(input, requires_grad=False).cuda()\n",
    "        target = Variable(target, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        # get a random minibatch from the search queue with replacement\n",
    "        input_search, target_search = next(iter(valid_queue))\n",
    "        input_search = Variable(input_search, requires_grad=False).cuda()\n",
    "        target_search = Variable(target_search, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        # Algorithm 1. Update undetermined architecture parameters(only alpha)\n",
    "        architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)\n",
    "        \n",
    "        # Algorithm 2. Update weights W\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        objs.update(loss.item(), n)\n",
    "        top1.update(prec1.item(), n)\n",
    "        top5.update(prec5.item(), n)\n",
    "        \n",
    "        msg = ('%10s' * 4) % (str(epoch),\n",
    "                              np.round(objs.avg, 4),\n",
    "                              np.round(top1.avg, 4),\n",
    "                              np.round(top5.avg, 4))\n",
    "        pbar.set_description(msg)\n",
    "            \n",
    "    return top1.avg, objs.avg\n",
    "\n",
    "def inference(valid_queue, model, criterion, epoch):\n",
    "    objs = utils.AverageMeter()\n",
    "    top1 = utils.AverageMeter()\n",
    "    top5 = utils.AverageMeter()\n",
    "    \n",
    "    pbar = enumerate(valid_queue)\n",
    "    print(('\\n' + '%10s' * 4) % ('Epoch', 'Loss', 'Top1', 'Top5'))\n",
    "    pbar = tqdm(pbar, total=len(valid_queue))\n",
    "    \n",
    "    model.eval()\n",
    "    for step, (input, target) in enumerate(valid_queue):\n",
    "        input = Variable(input).cuda()\n",
    "        target = Variable(target).cuda(async=True)\n",
    "\n",
    "        logits = model(input)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        n = input.size(0)\n",
    "        objs.update(loss.item(), n)\n",
    "        top1.update(prec1.item(), n)\n",
    "        top5.update(prec5.item(), n)\n",
    "        \n",
    "        msg = ('%10s' * 4) % (str(epoch),\n",
    "                              np.round(objs.avg, 4),\n",
    "                              np.round(top1.avg, 4),\n",
    "                              np.round(top5.avg, 4))\n",
    "        pbar.set_description(msg)\n",
    "\n",
    "    return top1.avg, objs.avg\n",
    "\n",
    "def edge_decision(type, alphas, selected_idxs, candidate_flags, probs_history, epoch, model, args):\n",
    "    mat = F.softmax(torch.stack(alphas, dim=0), dim=-1).detach()\n",
    "    \n",
    "    # Formula 1\n",
    "    importance = torch.sum(mat[:, 1:], dim=-1)\n",
    "\n",
    "    # Formula 2\n",
    "    probs = mat[:, 1:] / importance[:, None]\n",
    "    entropy = cate.Categorical(probs=probs).entropy() / math.log(probs.size()[1])\n",
    "\n",
    "\n",
    "    if args.use_history: # SGAS Cri.2 \n",
    "        # Formula 3\n",
    "        histogram_inter = histogram_average(probs_history, probs)\n",
    "        probs_history.append(probs)\n",
    "        if (len(probs_history) > args.history_size):\n",
    "            probs_history.pop(0)\n",
    "        \n",
    "        # Formula 5\n",
    "        score = utils.normalize(importance) * utils.normalize(\n",
    "            1 - entropy) * utils.normalize(histogram_inter)\n",
    "\n",
    "    else: # SGAS Cri.1\n",
    "        # Formula 4\n",
    "        score = utils.normalize(importance) * utils.normalize(1 - entropy)\n",
    "\n",
    "\n",
    "    if torch.sum(candidate_flags.int()) > 0 and \\\n",
    "            epoch >= args.warmup_dec_epoch and \\\n",
    "            (epoch - args.warmup_dec_epoch) % args.decision_freq == 0:\n",
    "        masked_score = torch.min(score,(2 * candidate_flags.float() - 1) * np.inf)\n",
    "        selected_edge_idx = torch.argmax(masked_score)\n",
    "        selected_op_idx = torch.argmax(probs[selected_edge_idx]) + 1 # add 1 since none op\n",
    "        selected_idxs[selected_edge_idx] = selected_op_idx\n",
    "\n",
    "        candidate_flags[selected_edge_idx] = False\n",
    "        alphas[selected_edge_idx].requires_grad = False\n",
    "        if type == 'normal':\n",
    "            reduction = False\n",
    "        elif type == 'reduce':\n",
    "            reduction = True\n",
    "        else:\n",
    "            raise Exception('Unknown Cell Type')\n",
    "        candidate_flags, selected_idxs = model.check_edges(candidate_flags,selected_idxs,reduction=reduction)\n",
    "        print(type + \"_candidate_flags {}\".format(candidate_flags))\n",
    "        score_image(type, score, epoch)\n",
    "        return True, selected_idxs, candidate_flags\n",
    "\n",
    "    else:\n",
    "        print(type + \"_candidate_flags {}\".format(candidate_flags))\n",
    "        score_image(type, score, epoch)\n",
    "        return False, selected_idxs, candidate_flags\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    torch.cuda.empty_cache()\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion, in_channel=1).cuda()\n",
    "    #print(sum(x.numel() for x in model.parameters()))\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        args.learning_rate,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay)\n",
    "    train_queue, valid_queue, test_queue = load_data(args)\n",
    "    \n",
    "    \n",
    "    num_edges = model._steps * 2\n",
    "    post_train = 5\n",
    "    epochs = args.warmup_dec_epoch + args.decision_freq * (num_edges - 1) + post_train + 1\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, float(epochs), eta_min=args.learning_rate_min)\n",
    "    print('num_edges:', num_edges, ' epochs:', epochs)\n",
    "    \n",
    "    architect = Architect(model, args)\n",
    "    \n",
    "    normal_selected_idxs = torch.tensor(len(model.alphas_normal) * [-1], requires_grad=False, dtype=torch.int).cuda()\n",
    "    reduce_selected_idxs = torch.tensor(len(model.alphas_reduce) * [-1], requires_grad=False, dtype=torch.int).cuda()\n",
    "    normal_candidate_flags = torch.tensor(len(model.alphas_normal) * [True], requires_grad=False, dtype=torch.bool).cuda()\n",
    "    reduce_candidate_flags = torch.tensor(len(model.alphas_reduce) * [True], requires_grad=False, dtype=torch.bool).cuda()\n",
    "    model.normal_selected_idxs = normal_selected_idxs\n",
    "    model.reduce_selected_idxs = reduce_selected_idxs\n",
    "    model.normal_candidate_flags = normal_candidate_flags\n",
    "    model.reduce_candidate_flags = reduce_candidate_flags\n",
    "    #print(F.softmax(torch.stack(model.alphas_normal, dim=0), dim=-1).detach())\n",
    "    #print(F.softmax(torch.stack(model.alphas_reduce, dim=0), dim=-1).detach())\n",
    "    \n",
    "    count = 0\n",
    "    normal_probs_history = []\n",
    "    reduce_probs_history = []\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        scheduler.step()\n",
    "        lr = scheduler.get_lr()[0]\n",
    "        \n",
    "        # train\n",
    "        train_acc, train_obj = train(train_queue, valid_queue, model, architect, criterion, optimizer, lr, epoch)\n",
    "        \n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            inference(test_queue, model, criterion, epoch)\n",
    "            \n",
    "        # greedy decision\n",
    "        saved_memory_normal, model.normal_selected_idxs, \\\n",
    "        model.normal_candidate_flags = edge_decision('normal',\n",
    "                                                     model.alphas_normal,\n",
    "                                                     model.normal_selected_idxs,\n",
    "                                                     model.normal_candidate_flags,\n",
    "                                                     normal_probs_history,\n",
    "                                                     epoch,\n",
    "                                                     model,\n",
    "                                                     args)\n",
    "\n",
    "        saved_memory_reduce, model.reduce_selected_idxs, \\\n",
    "        model.reduce_candidate_flags = edge_decision('reduce',\n",
    "                                                     model.alphas_reduce,\n",
    "                                                     model.reduce_selected_idxs,\n",
    "                                                     model.reduce_candidate_flags,\n",
    "                                                     reduce_probs_history,\n",
    "                                                     epoch,\n",
    "                                                     model,\n",
    "                                                     args)\n",
    "        \n",
    "        \n",
    "        if saved_memory_normal or saved_memory_reduce:\n",
    "            del train_queue, valid_queue\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            count += 1\n",
    "            new_batch_size = args.batch_size + args.batch_increase * count\n",
    "            train_queue = torch.utils.data.DataLoader(\n",
    "                train_data, batch_size=new_batch_size,\n",
    "                sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "                pin_memory=True, num_workers=2)\n",
    "\n",
    "            valid_queue = torch.utils.data.DataLoader(\n",
    "                train_data, batch_size=new_batch_size,\n",
    "                sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "                pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
